{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split the long description into manageable chunks\n",
    "def split_text(text, max_tokens):\n",
    "    \"\"\"Split the input text into chunks with a maximum token length.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    # Accumulate words until we reach the token limit\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(current_chunk) > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk[:-1]))  # Add current chunk (excluding last word)\n",
    "            current_chunk = [current_chunk[-1]]  # Start a new chunk with the last word\n",
    "    chunks.append(\" \".join(current_chunk))  # Add the last chunk\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088d48126a6a45f7b53ab619d1cb0624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,  # If you have GPU support\n",
    "    device_map=\"auto\",  # Automatically uses available GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example patent claim (Claim from Patent 1)\n",
    "claim_patent_1 = \"\"\"\n",
    "1. A method for wireless communication comprising: receiving a signal at a base station; processing the received signal \n",
    "   for decoding; and transmitting a decoded signal to a user equipment (UE).\n",
    "\"\"\"\n",
    "\n",
    "# Long patent description (Patent 2) - split this into chunks\n",
    "description_patent_2 = \"\"\"\n",
    "The base station performs signal processing using advanced modulation techniques, such as Quadrature Amplitude Modulation (QAM),\n",
    "to enhance the data rate. It is capable of handling high-speed data transmission, especially for urban areas with high traffic density.\n",
    "The user equipment (UE) is designed to support 5G NR (New Radio) frequencies and provides high throughput with low latency, even in crowded environments.\n",
    "Additional capabilities include supporting massive MIMO (Multiple Input Multiple Output) antennas for beamforming to increase network efficiency.\n",
    "Furthermore, the system can integrate with IoT (Internet of Things) devices for low-power communication in smart cities.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instruction template\n",
    "instruction_template = \"\"\"\n",
    "Based on the following claim from Patent 1, generate a new patent claim that incorporates the relevant features mentioned in the description of Patent 2.\n",
    "\n",
    "Claim from Patent 1: {claim}\n",
    "\n",
    "Description from Patent 2: {chunk}\n",
    "\n",
    "Print only the new claim with no notes or comments, just a raw claim with 450 words max. If you don't find any relevant features ignore the current chunk. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the long patent description into smaller chunks\n",
    "description_chunks = split_text(description_patent_2, max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A method for wireless communication comprising: receiving a signal at a base station; processing the received signal for decoding; transmitting a decoded signal to a user equipment (UE); and employing advanced modulation techniques such as Quadrature Amplitude Modulation (QAM) to enhance data rate, particularly in urban areas with high traffic density; and utilizing the user equipment (UE) to provide high throughput with low latency, even in crowded environments; and supporting massive MIMO (Multiple Input Multiple Output) antennas for beamforming to increase network efficiency; and integrating with IoT (Internet of Things) devices for low-power communication in smart cities.\n"
     ]
    }
   ],
   "source": [
    "# Process each chunk and generate new claims\n",
    "generated_claims = []\n",
    "for chunk in description_chunks:\n",
    "    instruction = instruction_template.format(claim=claim_patent_1, chunk=chunk)\n",
    "    outputs = pipe(instruction, max_new_tokens=500)  # Limit output length to prevent excessive text\n",
    "    generated_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "    # Try to remove the instruction and description from the generated output\n",
    "    # Assuming that the output is a continuation of the instruction, we will strip it\n",
    "    new_claim = generated_text.replace(instruction, \"\").strip()  # Remove the instruction part\n",
    "\n",
    "    # Append only the new claim to the list\n",
    "    generated_claims.append(new_claim)\n",
    "\n",
    "# Combine the generated claims from each chunk\n",
    "final_claim = \"\\n\".join(generated_claims)\n",
    "print(final_claim) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
